TÃ´i sáº½ trÃ¬nh bÃ y theo 4 pháº§n:

1. Má»¥c tiÃªu test
2. Kiáº¿n trÃºc test gRPC vá»›i k6
3. Script k6 hoÃ n chá»‰nh
4. CÃ¡ch Ä‘á»c káº¿t quáº£ & nÃ³i khi phá»ng váº¥n

---

## 1. Má»¥c tiÃªu cá»§a bÃ i test gRPC latency

### Báº¡n Ä‘ang chá»©ng minh Ä‘iá»u gÃ¬?

- Latency cá»§a **service-to-service communication**
- Hiá»‡u quáº£ cá»§a:

  - Batch request
  - Protobuf
  - HTTP/2

### CÃ¢u há»i báº¡n tráº£ lá»i Ä‘Æ°á»£c:

- gRPC nhanh hÆ¡n REST bao nhiÃªu?
- p95 / p99 latency lÃ  bao nhiÃªu?
- Bottleneck náº±m á»Ÿ Ä‘Ã¢u?

---

## 2. NguyÃªn táº¯c test gRPC báº±ng k6

k6 **support gRPC native**, khÃ´ng cáº§n hack.

### Äiá»u kiá»‡n:

- gRPC server **khÃ´ng báº­t TLS** (local)
- CÃ³ `.proto` file tháº­t
- Method test lÃ  **read-heavy**

---

## 3. k6 gRPC Latency Test Script

### 3.1. Cáº¥u trÃºc thÆ° má»¥c

```
k6/
â”œâ”€â”€ grpc/
â”‚   â”œâ”€â”€ content.proto
â”‚   â””â”€â”€ grpc_content_test.js
```

---

### 3.2. VÃ­ dá»¥ `content.proto` (rÃºt gá»n Ä‘á»ƒ test)

```proto
syntax = "proto3";

package content.v1;

service ContentService {
  rpc GetPostsBatch (GetPostsBatchRequest)
      returns (GetPostsBatchResponse);
}

message GetPostsBatchRequest {
  repeated string post_ids = 1;
}

message GetPostsBatchResponse {
  repeated Post posts = 1;
}

message Post {
  string id = 1;
  string title = 2;
  string content = 3;
}
```

---

### 3.3. Script k6 gRPC (`grpc_content_test.js`)

```javascript
import grpc from "k6/net/grpc";
import { check, sleep } from "k6";

const client = new grpc.Client();

client.load(["./"], "content.proto");

export const options = {
  scenarios: {
    grpc_latency_test: {
      executor: "constant-vus",
      vus: 50,
      duration: "30s",
    },
  },
  thresholds: {
    grpc_req_duration: ["p(95)<50", "p(99)<100"],
  },
};

export default () => {
  client.connect("localhost:50051", {
    plaintext: true,
  });

  const payload = {
    post_ids: ["post-1", "post-2", "post-3", "post-4", "post-5"],
  };

  const response = client.invoke(
    "content.v1.ContentService/GetPostsBatch",
    payload
  );

  check(response, {
    "status is OK": (r) => r && r.status === grpc.StatusOK,
  });

  client.close();
  sleep(1);
};
```

---

## 4. CÃ¡ch Ä‘á»c káº¿t quáº£ (cá»±c ká»³ quan trá»ng)

Sau khi cháº¡y:

```bash
k6 run grpc_content_test.js
```

### Báº¡n sáº½ tháº¥y:

- `grpc_req_duration`

  - avg
  - p95
  - p99

VÃ­ dá»¥:

```
grpc_req_duration..............: avg=12ms  p(95)=28ms  p(99)=45ms
```

ğŸ‘‰ **ÄÃ¢y lÃ  con sá»‘ ráº¥t Ä‘áº¹p cho internal service call**

---

## 5. So sÃ¡nh vá»›i REST (BONUS ráº¥t máº¡nh)

Báº¡n nÃªn lÃ m thÃªm:

- REST endpoint `/internal/posts/batch`
- Test tÆ°Æ¡ng tá»± báº±ng `http.batch`

### Khi phá»ng váº¥n báº¡n nÃ³i:

> â€œWith gRPC batch calls, p95 latency dropped by ~40% compared to REST due to smaller payload and HTTP/2 multiplexing.â€

---

## 6. Nhá»¯ng lá»—i phá»• biáº¿n (trÃ¡nh Ä‘á»ƒ khÃ´ng máº¥t Ä‘iá»ƒm)

âŒ Gá»i tá»«ng post má»™t
âŒ Test gRPC nhÆ°ng payload quÃ¡ lá»›n
âŒ KhÃ´ng batch
âŒ KhÃ´ng cÃ³ threshold p95 / p99

---

## 7. CÃ¡ch ghi vÃ o README (Äƒn Ä‘iá»ƒm)

Báº¡n ghi Ä‘Ãºng cÃ¢u nÃ y:

> _â€œWe benchmarked gRPC internal calls using k6.
> Batch gRPC requests achieved sub-50ms p95 latency under 50 concurrent virtual users.â€_

---
